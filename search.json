[
  {
    "objectID": "preregistration_modelling.html",
    "href": "preregistration_modelling.html",
    "title": "Adaptive Preregistration for Modelling",
    "section": "",
    "text": "Rationale & Explanation\n\n\n\nThis section specifies the decision-making context in which the model will be used or the intended scope and context of conclusions. Important components include the decision maker and stakeholders (including experts) and their view on: i) the nature of the problem or decision addressed and how the scope of the modelling tool fits within the (broader) context (i.e. model purpose; ii) the spatial and temporal scales relevant to the decision context; iii) specified desired outputs; iv) role and inclusion in model development and testing; v) whether they foresee unacceptable outcomes that need to be represented in the model (i.e. as constraints), and; vi) what future scenarios does the model need to account for (noting this may be revised later). It should also provide a summary of the domain of applicability of the model, and reasonable extrapolation limits (Grimm et al. 2014).\n\n\n\n\n\n\n\n\n\n\nRationale & Explanation\n\n\n\nDefining the purpose of the model is critical because the model purpose influences choices at later stages of model development (Jakeman, Letcher, and Norton 2006). Common model purposes in ecology include: gaining a better qualitative understanding of the target system, synthesising and reviewing knowledge, and providing guidance for management and decision-making (Jakeman, Letcher, and Norton 2006). Note that modelling objectives are distinct from the analytical objectives of the model.\n\n\n\n\n\n\n\n\n\n\nPreregistration Item\n\n\n\n\nProvide a clear statement of the modelling objectives and problem that the model seeks to illuminate. What is the purpose of the model/s? Ensure that you specify any focal taxa and study objectives, as well as any clients for whom the model is developed. Briefly outline the ecological problem and the decision-problem, including the decision-trigger and any regulatory frameworks relevant to the problem.\n\n\n\n\n\n\n\n\n\n\n\n\n\nRationale & Explanation\n\n\n\nThe scope of the model, including temporal and spatial resolutions are defined here (Mahmoud et al. 2009), and any limitations on model development analysis and flexibility should be outlined in this section (Jakeman, Letcher, and Norton 2006). Note that the modelling context is different from the problem context which are described in 1.3.1.\n\n\n\n\n\n\n\n\n\n\nPreregistration Item\n\n\n\nIdentify interest groups in the modelling context. This includes clients, and end-users of the model. Who is the model for, who is involved in formulating the model? Who needs buy in? Describe the decision-making context in which the model will be used.\n\n\n\n\n\nDetermine the temporal and spatial scope of the model. Where is the boundary of the modelled system? Everything outside beyond the boundary and not crossing it is to be ignored within the domain of the model, and everything crossing the boundary is to be treated as external forcing (known/unknown), or else as model outputs (observed, or not, Jakeman et al. 2006) HF comment I dont understand the preceeding sentence, the concept of boundaries of the model is foreign to me. Also, is this question more to do with describing the scope and limitations of the data rather then the model?*. The choice of a model’s boundaries is closely linked to the choice of how finely to aggregate the behaviour within the model (Jakeman et al., 2006) - what is the intended scale and resolution of the model (temporal, spatial or otherwise)?\nExplain how any key concepts or terms within problem or decision-making contexts, such as regulatory terms, will be operationalised and defined in a biologically meaningful way to answer the research question appropriately? (Should this last step go here or within define conceptual framework?. HF comment. I think it belongs with the conceptual framework).\nWhat is the intended domain of applicability of the model, what is the extent of acceptable extrapolations (Grimm)? This is relevant to model transferability… (similar to a COG statement..)* A comment:This section feels a bit too big and confusing from my perspective and I do think you might need to move some of it to a different section to reduce the risk of misinterpretation. I’m just thinking about how I might fill this out for an imagined study. The literal boundaries of the study are usually straightforward, but the theoretic boundaries are a different and also big thing to try to conceptualise in one section. You could have a section for the spatial/temporal boundaries of the study, and another section on how far the results can be extrapolated based on the study design. This makes sense to me in how I design experiments, if I have too many confounding variables and not enough spatial and environmental replication then I’m less likely to make any broad claims. However, if I have controlled or experimentally manipulated factors then I feel safer making bigger generalisations about results.\n\n\n\nTime-frame - When must the model by completed, e.g. to help make a decision?\nWhat effort and resources are available for both developing the model and operating the model? HF comment; as written this might be slightly confronting because you could interpret this to include writing explictly about budget constraints which might be confidential. Maybe this could be alleviated with with an example?\nWhat degree of flexibility is required from the model? Might the model need to be quickly reconfigured to explore new scenarios or problems proposed by clients / managers / model-users?\n\n\n\n\nHow will the model be analysed, what analytical questions will the model be used to answer? Examples from ecological decision-making include: to compare the performance of alternative management actions under budget constraint (Fraser et al. 2017) to search for robust decisions under uncertainty (McDonald-Madden, Baxter, and Possingham 2008), to choose the conservation policy that minimises uncertainty [insert ref]. See other examples in (Moallemi, Elsawah, and Ryan 2019).\n\n\nThe outcomes should speak directly to the analytical objectives identified in 1.3.1. Outcomes could be qualitative or quantitative. For example, you might be using your model in a scenario analysis to determine which management decision is associated with minimum regret or the highest likelihood of improvement. To be honest, I don’t know how I might apply this to my work or any future works. Maybe I do not have enough knowledge of this type of research to apply it to my basic ecology research. But this isn’t to say it’s not important, I just wonder if there’s a way to make this accessible to non-decision or modelling peeps. Like, how would I apply it to the woodland condition analysis?\n\n\n\n\nCandidate decisions should be investigated and are specified a priori. Depending on the modelling context, they may be specified by stakeholders, model users or the analyst(Moallemi, Elsawah, and Ryan 2019). Describe the method used to identify relevant management actions.\n\n\n\nDescribe what processes you will use to elicit and identify relevant scenarios, e.g. literature review, structured workshops with stakeholders or decision-makers. Specify scenarios under which decisions are investigated. Scenarios should be set a priori (i.e. before the model is built, Moallemi, Elsawah, and Ryan 2019) and may be stakeholder-defined or driven by the judgement of the modeller or other experts (Mahmoud et al. 2009)."
  },
  {
    "objectID": "preregistration_modelling.html#model-purpose",
    "href": "preregistration_modelling.html#model-purpose",
    "title": "Adaptive Preregistration for Modelling",
    "section": "",
    "text": "Rationale & Explanation\n\n\n\nDefining the purpose of the model is critical because the model purpose influences choices at later stages of model development (Jakeman, Letcher, and Norton 2006). Common model purposes in ecology include: gaining a better qualitative understanding of the target system, synthesising and reviewing knowledge, and providing guidance for management and decision-making (Jakeman, Letcher, and Norton 2006). Note that modelling objectives are distinct from the analytical objectives of the model.\n\n\n\n\n\n\n\n\n\n\nPreregistration Item\n\n\n\n\nProvide a clear statement of the modelling objectives and problem that the model seeks to illuminate. What is the purpose of the model/s? Ensure that you specify any focal taxa and study objectives, as well as any clients for whom the model is developed. Briefly outline the ecological problem and the decision-problem, including the decision-trigger and any regulatory frameworks relevant to the problem."
  },
  {
    "objectID": "preregistration_modelling.html#specify-modelling-context",
    "href": "preregistration_modelling.html#specify-modelling-context",
    "title": "Adaptive Preregistration for Modelling",
    "section": "",
    "text": "Rationale & Explanation\n\n\n\nThe scope of the model, including temporal and spatial resolutions are defined here (Mahmoud et al. 2009), and any limitations on model development analysis and flexibility should be outlined in this section (Jakeman, Letcher, and Norton 2006). Note that the modelling context is different from the problem context which are described in 1.3.1.\n\n\n\n\n\n\n\n\n\n\nPreregistration Item\n\n\n\nIdentify interest groups in the modelling context. This includes clients, and end-users of the model. Who is the model for, who is involved in formulating the model? Who needs buy in? Describe the decision-making context in which the model will be used.\n\n\n\n\n\nDetermine the temporal and spatial scope of the model. Where is the boundary of the modelled system? Everything outside beyond the boundary and not crossing it is to be ignored within the domain of the model, and everything crossing the boundary is to be treated as external forcing (known/unknown), or else as model outputs (observed, or not, Jakeman et al. 2006) HF comment I dont understand the preceeding sentence, the concept of boundaries of the model is foreign to me. Also, is this question more to do with describing the scope and limitations of the data rather then the model?*. The choice of a model’s boundaries is closely linked to the choice of how finely to aggregate the behaviour within the model (Jakeman et al., 2006) - what is the intended scale and resolution of the model (temporal, spatial or otherwise)?\nExplain how any key concepts or terms within problem or decision-making contexts, such as regulatory terms, will be operationalised and defined in a biologically meaningful way to answer the research question appropriately? (Should this last step go here or within define conceptual framework?. HF comment. I think it belongs with the conceptual framework).\nWhat is the intended domain of applicability of the model, what is the extent of acceptable extrapolations (Grimm)? This is relevant to model transferability… (similar to a COG statement..)* A comment:This section feels a bit too big and confusing from my perspective and I do think you might need to move some of it to a different section to reduce the risk of misinterpretation. I’m just thinking about how I might fill this out for an imagined study. The literal boundaries of the study are usually straightforward, but the theoretic boundaries are a different and also big thing to try to conceptualise in one section. You could have a section for the spatial/temporal boundaries of the study, and another section on how far the results can be extrapolated based on the study design. This makes sense to me in how I design experiments, if I have too many confounding variables and not enough spatial and environmental replication then I’m less likely to make any broad claims. However, if I have controlled or experimentally manipulated factors then I feel safer making bigger generalisations about results.\n\n\n\nTime-frame - When must the model by completed, e.g. to help make a decision?\nWhat effort and resources are available for both developing the model and operating the model? HF comment; as written this might be slightly confronting because you could interpret this to include writing explictly about budget constraints which might be confidential. Maybe this could be alleviated with with an example?\nWhat degree of flexibility is required from the model? Might the model need to be quickly reconfigured to explore new scenarios or problems proposed by clients / managers / model-users?"
  },
  {
    "objectID": "preregistration_modelling.html#explain-analytical-objectives",
    "href": "preregistration_modelling.html#explain-analytical-objectives",
    "title": "Adaptive Preregistration for Modelling",
    "section": "",
    "text": "How will the model be analysed, what analytical questions will the model be used to answer? Examples from ecological decision-making include: to compare the performance of alternative management actions under budget constraint (Fraser et al. 2017) to search for robust decisions under uncertainty (McDonald-Madden, Baxter, and Possingham 2008), to choose the conservation policy that minimises uncertainty [insert ref]. See other examples in (Moallemi, Elsawah, and Ryan 2019).\n\n\nThe outcomes should speak directly to the analytical objectives identified in 1.3.1. Outcomes could be qualitative or quantitative. For example, you might be using your model in a scenario analysis to determine which management decision is associated with minimum regret or the highest likelihood of improvement. To be honest, I don’t know how I might apply this to my work or any future works. Maybe I do not have enough knowledge of this type of research to apply it to my basic ecology research. But this isn’t to say it’s not important, I just wonder if there’s a way to make this accessible to non-decision or modelling peeps. Like, how would I apply it to the woodland condition analysis?"
  },
  {
    "objectID": "preregistration_modelling.html#define-candidate-decisions-and-method-of-identifying-relevant-management-actions",
    "href": "preregistration_modelling.html#define-candidate-decisions-and-method-of-identifying-relevant-management-actions",
    "title": "Adaptive Preregistration for Modelling",
    "section": "",
    "text": "Candidate decisions should be investigated and are specified a priori. Depending on the modelling context, they may be specified by stakeholders, model users or the analyst(Moallemi, Elsawah, and Ryan 2019). Describe the method used to identify relevant management actions."
  },
  {
    "objectID": "preregistration_modelling.html#specify-scenarios",
    "href": "preregistration_modelling.html#specify-scenarios",
    "title": "Adaptive Preregistration for Modelling",
    "section": "",
    "text": "Describe what processes you will use to elicit and identify relevant scenarios, e.g. literature review, structured workshops with stakeholders or decision-makers. Specify scenarios under which decisions are investigated. Scenarios should be set a priori (i.e. before the model is built, Moallemi, Elsawah, and Ryan 2019) and may be stakeholder-defined or driven by the judgement of the modeller or other experts (Mahmoud et al. 2009)."
  },
  {
    "objectID": "preregistration_modelling.html#choose-elicitation-and-representation-method",
    "href": "preregistration_modelling.html#choose-elicitation-and-representation-method",
    "title": "Adaptive Preregistration for Modelling",
    "section": "2.1 2.1 Choose elicitation and representation method",
    "text": "2.1 2.1 Choose elicitation and representation method\nDescribe what method you will use to elicit or identify the conceptual model. Some common methods include interviews, drawings, and mapping techniques including influence diagrams, cognitive maps and Bayesian belief networks (Moon et al. 2019). (Libby, to provide link to any structured expert elicitation methods?).While it is difficult to decide and justify which method is most appropriate, however Moon et al. (2019) provide guidance addressing this methodological question. Finally, how do you intend on representing the final conceptual model? This will likely depend on the method chosen to elicit the conceptual model."
  },
  {
    "objectID": "preregistration_modelling.html#explain-critical-conceptual-design-decisions",
    "href": "preregistration_modelling.html#explain-critical-conceptual-design-decisions",
    "title": "Adaptive Preregistration for Modelling",
    "section": "2.2 2.2.1 Explain Critical Conceptual Design Decisions",
    "text": "2.2 2.2.1 Explain Critical Conceptual Design Decisions\nThis step should list and explain the critical conceptual design decisions, including: spatial and temporal scales, selection of entities and processes, representation of stochasticity and heterogeneity, consideration of local versus global interactions, environmental drivers, etc. (Grimm et al. 2014). The influence of particular theories, concepts, or importantly, earlier models, should be explained and justified against alternative conceptual design decisions that might lead to alternative model structures (Grimm et al. 2014)."
  },
  {
    "objectID": "preregistration_modelling.html#specify-key-assumptions-and-uncertainties-underlying-the-models-design.",
    "href": "preregistration_modelling.html#specify-key-assumptions-and-uncertainties-underlying-the-models-design.",
    "title": "Adaptive Preregistration for Modelling",
    "section": "2.3 2.2.2 Specify key assumptions and uncertainties underlying the model’s design.",
    "text": "2.3 2.2.2 Specify key assumptions and uncertainties underlying the model’s design.\n\nDescribe how uncertainty and variation will be represented in this model. This includes both exogenous uncertainties affecting the system, parametric uncertainty in input data and structural / conceptual nonparametric uncertainty in the model (Moallemi, Elsawah, and Ryan 2019).*"
  },
  {
    "objectID": "preregistration_modelling.html#identify-predictor-and-response-variables",
    "href": "preregistration_modelling.html#identify-predictor-and-response-variables",
    "title": "Adaptive Preregistration for Modelling",
    "section": "2.4 2.3 Identify predictor and response variables",
    "text": "2.4 2.3 Identify predictor and response variables\nThe identification and definition of primary model input variables should be driven by scenario definitions, and by the scope of the model described in the problem formulation phase (Mahmoud et al. 2009) – Identify and define system variables structures:\n\nWhat variables would support taking this action or making this decision?\nWhat additional variables may interact with this system (things we can’t control, but can hopefully measure)?\nWhat variables have not been measured, but may interact with the system (often occurs in field or observational studies)?\nWhat variables are indice or surrogate measures of variables that we cannot or have not measured?\nIn what ways do we expect these variables to interact (model structures)?"
  },
  {
    "objectID": "preregistration_modelling.html#define-prior-knowledge-data-specification-and-evaluation",
    "href": "preregistration_modelling.html#define-prior-knowledge-data-specification-and-evaluation",
    "title": "Adaptive Preregistration for Modelling",
    "section": "2.5 2.4 Define prior knowledge, data specification and evaluation",
    "text": "2.5 2.4 Define prior knowledge, data specification and evaluation\nCollect, process and prepare data available for parameterisation, determining model structure, and for scenario analysis.\n\n2.5.1 2.4.1 Collate available data sources that could be used to parameterise or structure the model\nDocument the identity, quantity and provenance of any data that will be used to develop, identify and test the model. Describe how the data is arranged, in terms of replicates and covariates.\nFor pre-existing data: – delete as appropriate.\n\nFor each dataset, is the data open or publically available? (Y/N)\nHow can the data be accessed? Provide a link or contact as appropriate, indicating any restrctions on the use of data.\nDate of download, access, or future access:\nDescribe the source of the data - what entity originally collected this data? (National Data Set, Private Organisational Data, Own Lab Collection, Other Lab Collection, External Contractor, Meta-Analysis, Expert Elicitation, Other).\nCodebook and meta-data. If a Codebook or other meta-data is available, please link to it here and / or upload the document(s).\nPrior work based on this dataset - Have you published / presented any previous work based on this dataset? Include any publications, conference presentations (papers, posters), or working papers (in-prep, unpublished, preprints) based on this dataset you have worked on. HF- Love this question\nUnpublished Prior Research Activity - Describe any prior but unpublished research activity using these data. Be specific and transparent.\nPrior knowledge of the current dataset - Describe any prior knowledge of or interaction with the dataset before commencing this study. For example, have you read any reports or publications about this data?\n\nSampling Plan (For data you will collect) – delete as appropriate.\n\nData collection procedures - Please describe your data collection process, including how sites and transects or any other physical unit were selected and arranged. Describe any inclusion or exclusion rules, and the study timeline.\nSample Size - Describe the sample size of your study.\nSample Size Rationale - Describe how you determined the appropriate sample size for your study. It could include feasibility constraints, such as time, money or personnel.\nIf sample size cannot be specified, specify a stopping rule - i.e. how will you decide when to terminate your data collection?\n\n\n\n2.5.2 2.4.2 Data Processing and Preparation\nDescribe any data preparation and processing steps, including manipulation of environmental layers, e.g. standardisation and geographic projection.\nDescribe how you will separate and distinguish between raw data, manipulated data, and outputs from modelling or any analyses of the model.\n\n\n2.5.3 2.4.3 Describe any data exploration or preliminary data analyses.\nIn most modelling cases, it is necessary to perform preliminary analyses to understand the data and check that assumptions and requirements of the chosen modelling procedures are met. Data exploration prior to model fitting or development may include exploratory analyses to check for collinearity, spatial and temporal coverage, quality and resolution, outliers, or the need for transformations (Yates et al. 2018). Describe how you will summarise and explore your data, and explain what method you will use (graphical, tabular or otherwise) to represent your data and any analyses. Specify what you intend to find out or hope to learn from each data exploration analysis.\n\n\n2.5.4 2.4.4 Data Evaluation\nDescribe how you will determine how reliable the data is for the given purpose. Ideally, model input data should be internally consistent across temporal and spatial scales and resolutions, and appropriate to the problem at hand (Mahmoud et al. 2009). ### 2.4.5 Data Evaluation: Data reliability reporting ### 2.4.5. Data Evaluation: data reliability reporting\nDocument any issues with data reliability. This is important because data quality and ecological relevance might be constrained by measurement error, inappropriate experimental design, and heterogeneity and variability inherent in ecological systems (Grimm et al. 2014).\n\n\n2.5.5 2.4.6. Data evaluation: data exclusion\nHow will you determine what data, if any, will be excluded from your analyses? How will outliers be handled? Describe rules for identifying outlier data, and for excluding a site, transect, quadrat, year or season, species, trait, etc.\n\n\n2.5.6 2.4.7 Data Evaluation: Missing Data\nHow will you identify and deal with incomplete or missing data?"
  },
  {
    "objectID": "preregistration_modelling.html#conceptual-model-evaluation",
    "href": "preregistration_modelling.html#conceptual-model-evaluation",
    "title": "Adaptive Preregistration for Modelling",
    "section": "2.6 2.5 Conceptual model evaluation",
    "text": "2.6 2.5 Conceptual model evaluation\nDescribe how your conceptual model will be critically evaluated. Evaluation includes both the completeness and suitability of the overall model structure.\nHow will any simplifying assumptions be critically assessed (Augusiak, Van den Brink, and Grimm 2014)?\nExplain whether this process will include consultation or feedback from a client, manager, or model user."
  },
  {
    "objectID": "preregistration_modelling.html#choose-model-class-framework-and-approach",
    "href": "preregistration_modelling.html#choose-model-class-framework-and-approach",
    "title": "Adaptive Preregistration for Modelling",
    "section": "3.1 3.1 Choose model class, framework and approach",
    "text": "3.1 3.1 Choose model class, framework and approach\nDescribe what class or approach of model you will use and¬†explain how the choice of model class was informed by the analytical objectives of the model. Modelling approaches lie on a spectrum from correlative or phenomenological to mechanistic or process-based¬† (Yates et al. 2018); where correlative models use mathematical functions fitted to data to describe underlying processes, and mechanistic models explicitly represent processes and details of component parts of a biological system that are expected to give rise to the data (White and Marshall 2019)."
  },
  {
    "objectID": "preregistration_modelling.html#choose-model-features-and-family",
    "href": "preregistration_modelling.html#choose-model-features-and-family",
    "title": "Adaptive Preregistration for Modelling",
    "section": "3.2 3.2 Choose model features and family",
    "text": "3.2 3.2 Choose model features and family\nAll modelling approaches require the selection of model features, which conform with the conceptual model and data specified in previous steps (Jakeman 2006). The choice of model are determined in conjunction with features are selected. Usually difficult to change fundamental features of a model beyond an early stage of model development, so careful thought and planning here is useful to the modeller (Jakeman, 2006). However, if changes to these fundamental aspects of the model do need to change, please document how and why these choices were made. HF- it’s not clear to me what a model feature is and I find this a little confusing. I wonder if we need the text describing the 3.2 heading when it’s stepped out in the subsections.\n\n3.2.1 3.2.1 Explain how you will operationalise response variable(s)\nSpecify how you will operationalise the response variables in the model. This should relate directly to the analytical and or management objectives specified during the problem formulation phase. Operationalisations could include:- the extent of a response- an extreme value- a trend- a long-term mean- a probability distribution- a spatial pattern- a time-series, - qualitative change, such as a direction of change or- the frequency, location, or probability of some event occuring. Provide a rationale for your choices, including why plausible alternatives were not chosen (Jakeman, 2006). HF- this is great. Only slight concern is the deceptively difficult task that last sentence represents\n\n\n3.2.2 3.2.2 Choose model family\nSpecify which family of statistical distributions you will use in your model, and describe any transformations, or link functions. Justify your decision based on the purpose, objectives, prior knowledge and logistical constraints (Jakeman, 2006) specified in the problem formulation phase.\nInclude in your rational for selection, detail about which variables the model outputs are sensitive to, what aspects of their behaviour are important, and any associated spatial or temporal dimensions in sampling.\n\n\n3.2.3 3.2.3 Choose model features\nSpecify which variables are included in the model, and the nature of their treatment (e.g. lumped/distributed, linear/non-linear, stochastic/deterministic, Jakeman, 2006).\nSpecify model structural features, including: - the functional form of interactions, - data structures - measures used to specify links, - any bins or discretisation of continuous variables."
  },
  {
    "objectID": "preregistration_modelling.html#choose-approach-for-identifying-model-structure-and-parameters",
    "href": "preregistration_modelling.html#choose-approach-for-identifying-model-structure-and-parameters",
    "title": "Adaptive Preregistration for Modelling",
    "section": "3.3 3.3 Choose approach for identifying model structure and parameters",
    "text": "3.3 3.3 Choose approach for identifying model structure and parameters\nThis refers to the process of determining the best/most efficient/parsimonious representation of the system at the appropriate scale of concern (Jakeman, 2006) that best meets the analytical objectives specified in the problem formulation phase. Approaches to finding model structure and parameters may be knowledge-supported, or data-driven (Boets et al. 2015). Model selection methods can include traditional inferential approaches such as unconstrained searches of a dataset for patterns that explain variations in the response variable, or use of ensemble-modelling methods (Barnard et al. 2019). Ensemble modelling procedures might aim to derive a single model, or a multi-model average (Yates et al. 2018). Refining actions to develop a model could include iteratively dropping parameters or adding them, or aggregating / disaggregating system descriptors, such as dimensionality and processes (Jakeman, 2006). Specify what approach and methods you will use to identify model structure and parameters."
  },
  {
    "objectID": "preregistration_modelling.html#choose-estimation-technique-and-performance-criteria",
    "href": "preregistration_modelling.html#choose-estimation-technique-and-performance-criteria",
    "title": "Adaptive Preregistration for Modelling",
    "section": "3.4 3.4 Choose estimation technique and performance criteria",
    "text": "3.4 3.4 Choose estimation technique and performance criteria\nBefore calibrating the model to the data, the performance criteria on which the calibration is judged are chosen. These criteria and their underlying assumptions should reflect the desired properties of the parameter estimates / structure (Jakeman, Letcher, and Norton 2006). For example, modellers might seek parameter estimates that are robust to outliers, unbiased, and yield appropriate predictive performance. Modellers will need to consider whether the assumptions of the estimation technique yielding those desired criteria are appropriate to the problem at hand. For integrated or sub-divided models, other considerations might include choices about where to disaggregate the model for parameter estimation; e.g. spatial sectioning (streams into reaches) and temporal sectioning (piece-wise linear models) (Jakeman, Letcher, and Norton 2006). Specifying performance criteria a priori is important because it involves pre-specifying how the fitted or quantitative model will be interpreted in advance, avoiding biases like confirmation bias, HARKing, and cherry-picking some performance tests from the full suite of tests undertaken. HF- I think this paragraph is only relevant to performance criteria- doesn’t really explore estimation technique. As above though, I’m not sure that this paragraph is neessary given that it’s stepped out in the subheadings below\n\n3.4.1 3.4.1 Choice of performance criteria\nSpecify which suite of performance criteria you will use to judge the performance of the model. Examples include correlation scores, coefficient of determination, specificity, sensitivity, AUC, etcetera (Yates et al. 2018). Relate any underlying assumptions of each criterion to the desired properties of the model, and justify the choice of performance metric in relation into whether it is sensitive to the problem at hand. Explain how you will identify which model features or components are significant or meaningful.\n\n\n3.4.2 3.4.2 Choice of parameter estimation technique\nHF- I think that it might be double dipping to include structure here as well as in 3.3 Specify what technique you will use to estimate parameter values, and how you will supply non-parametric variables and/or data (e.g. distributed boundary conditions). For example, will you calibrate all variables simultaneously by optimising fit of model outputs to observations, or will you parameterise the model in a piecemeal fashion by either direct measurement, inference from secondary data, or some combination (Jakeman, 2006). Identify which variables were parameterised directly, such as by expert elicitation or prior knowledge. Specify which algorithm(s) you will use for any data-driven parameter estimation, including supervised, or unsupervised machine learning, decision-tree, K-nearest neighbour or cluster algorithms (Liu et al. 2018)."
  },
  {
    "objectID": "preregistration_modelling.html#specify-model-assumptions-and-uncertainties",
    "href": "preregistration_modelling.html#specify-model-assumptions-and-uncertainties",
    "title": "Adaptive Preregistration for Modelling",
    "section": "3.5 3.5 Specify model assumptions and uncertainties",
    "text": "3.5 3.5 Specify model assumptions and uncertainties\nSpecify all assumptions and key uncertainties in the formal model. Describe what gaps exist between the model conception, and the real-world problem, what biases might this introduce and how might this impact any interpretation of the model outputs, and what implications are there on evaluating model-output to inform decisions?"
  },
  {
    "objectID": "preregistration_modelling.html#specify-formal-model",
    "href": "preregistration_modelling.html#specify-formal-model",
    "title": "Adaptive Preregistration for Modelling",
    "section": "3.6 3.6 Specify formal model",
    "text": "3.6 3.6 Specify formal model\nOnce critical decisions have been made about the approach and method of model specification,translate the conceptual model into the quantitative model. For data-driven and model-selection approaches that determine model structure and parameters, describe any initial model specifications and parameterisations, including for any tune-in parameters. (Should this go here or in model calibration?)"
  },
  {
    "objectID": "preregistration_modelling.html#model-calibration-and-validation-scheme",
    "href": "preregistration_modelling.html#model-calibration-and-validation-scheme",
    "title": "Adaptive Preregistration for Modelling",
    "section": "4.1 Model calibration and validation scheme",
    "text": "4.1 Model calibration and validation scheme\nDescribe how you will validate and check the calibration of the model. The model may be tested on data independent of those used to parameterise the model (external validation), or the model may be cross-validated on random sub-samples of the data used to parameterise the model (internal cross-validation) (Yates et al. 2018; Barnard et al. 2019). Justify your choices.\n\n4.1.1 Describe validation data\nIf partitioning data for cross-validation: describe the approach specifying the number of folds that will be created and the relative size of each. Describe how will you document and/or share the partitioned data such that the data partitioning and any subsequent modelling based on this partitioning can be computationally reproduced.\nIf using external data: describe any known differences between the training and validation datasets and the size of the validation dataset. Describe how will you document and/or share the external data such that any subsequent modelling can be computationally reproduced."
  },
  {
    "objectID": "preregistration_modelling.html#implementation-verification",
    "href": "preregistration_modelling.html#implementation-verification",
    "title": "Adaptive Preregistration for Modelling",
    "section": "4.2 Implementation verification",
    "text": "4.2 Implementation verification\nWhat Quality Assurance measures will you take to verify the model has been correctly implemented? Tests could include syntax checking of code, and code reviews by peers. Checks for verification implementation should include i) thoroughly checking for bugs or programming errors, and ii) whether the implemented model performs as dictated by the model description (Grimm et al. 2014). Specifying up front quality assurance tests for implementation verification may help to avoid selective debugging."
  },
  {
    "objectID": "preregistration_modelling.html#model-checking",
    "href": "preregistration_modelling.html#model-checking",
    "title": "Adaptive Preregistration for Modelling",
    "section": "4.3 Model checking",
    "text": "4.3 Model checking\n\n\n\n\n\n\nRationale & Explanation\n\n\n\n“Model Checking” goes by many names (“conditional verification”, “quantitative verification”, “model output verification” ), and refers to a series of analyses that assess a model’s performance in representing the system of interest (Conn et al. 2018). Model checking aids in diagnosing assumption violations, and reveals where a model might need to be altered to better represent the data, and therefore system (Conn et al. 2018). Quantitative model checking diagnostics include goodness of fit, tests on residuals or errors, such as for heteroscedascity, cross-correlation, and autocorrelation (Jakeman, Letcher, and Norton 2006).\n\n\n\n4.3.1 Quantitative Model Checking\n\n\n\n\n\n\nPreregistration Item\n\n\n\nDuring this process, observed data, or data and patterns that guided model design and calibration is compared to model output in order to identify if and where there are any systematic differences.\n\nSpecify any diagnostics or tests you will use during model checking to assess a model’s performance in representing the system of interest.\nFor each test, specify the criteria that will you use to interpret the outcome of the test in assessing the model’s ability to sufficiently represent the gathered data used to develop and parameterise the model.\n\n\n\n\n\n4.3.2 Qualitative Model Checking\n\n\n\n\n\n\nExplanation\n\n\n\nThis step is largely informal and case-specific, but requires‚ ‘face validation’ with model users / clients / managers who aren’t involved in the development of the model to assess whether the interactions and outcomes of the model are feasible an defensible (Grimm et al. 2014). This process is sometimes called a ‚“laugh test” or a “pub test” and in addition to checking the model’s believability, it builds the client’s confidence in the model (Jakeman, Letcher, and Norton 2006). Face validation could include structured walk-throughs, or presenting descriptions, visualisations or summaries of model results to experts for assessment.\n\n\n\n\n\n\n\n\nPreregistration Item\n\n\n\n\nBriefly explain how you will qualitatively check the model, and whether and how you will include users and clients in the process.\n\n\n\n\n\n4.3.3 Assumption Violation Checks\n\n\n\n\n\n\nCaution\n\n\n\nThe consequences of assumption violations on the interpretation of results should be assessed (Araújo et al. 2019).\n\nExplain how you will demonstrate robustness to model assumptions and check for violations of model assumptions.\nIf you cannot perform quantitative assumption checks, describe what theoretical justifications would justify a lack of violation of or robustness to model assumptions.\nIf you cannot demonstrate or theoretically justify violation or robustness to assumptions, explain why not, and specify whether you will discuss assumption violations and their consequences for interpretation of model outputs.\nIf assumption violations cannot be avoided, explain how you will explore the consequences of assumption violations on the interpretation of results (To be completed in interim iterations of the preregistration, only if there are departures from assumptions as demonstrated in the planned tests above)."
  },
  {
    "objectID": "preregistration_modelling.html#model-output-corroboration",
    "href": "preregistration_modelling.html#model-output-corroboration",
    "title": "Adaptive Preregistration for Modelling",
    "section": "5.1 5.1 Model output corroboration",
    "text": "5.1 5.1 Model output corroboration\nIdeally, model outputs or predictions are compared to independent data and patterns that were not used to develop, parameterise, or verify the model. Testing against a dataset of response and predictor variables that are sptially and/or temporally independent from the training dataset¬†minimises the risk of artificially inflating model performance measures (Araújo et al. 2019).State whether you will corroborate the model outputs on external data, and please document any independent validation data in step &lt;2.4&gt;.It is preferable that any independent data used for model evaluation remains unknown to modellers during the process of model building, please describe the relationship modllers have to model validation data, will independent datasets be known to any modeller or analyst involved in the model building process?Although the corroboration of model outputs against an independent validation dataset is considered the ‘gold standard’ for showing that a model properly represents the internal organisation of the system ), model validation is not always possible because empirical experiments are infeasible or model users are working on rapid-response time-frames ‚Äî hence, why ecologists often model in the first place (Grimm, et al. 2014. Independent predictions can instead be tested on submodels. Alternatively, patterns in model output that are robust and seem characteristic of the system can be identified and evaluated in consultation with the literature or by experts to judge how accurate the model’s output is (Grimm et al.2014). If unable to evaluate the model outputs against independent data, explain why and explain what steps you will take to interrogate the model."
  },
  {
    "objectID": "preregistration_modelling.html#choose-performance-metrics-and-criteria",
    "href": "preregistration_modelling.html#choose-performance-metrics-and-criteria",
    "title": "Adaptive Preregistration for Modelling",
    "section": "5.2 5.2 Choose performance metrics and criteria",
    "text": "5.2 5.2 Choose performance metrics and criteria\nModel performance can be quantified by a range of tests, including measures of agreement between predictions and independent observations, or estimates of accuracy, bias, calibration, discrimination refinement, resolution and skill (Araújo et al. 2019). Specify what performance measures you will use to evaluate the model and briefly explain how each test relates to different desired properties of a model’s performance.Spatial, temporal and environmental pattern of errors and variance can change the interpretation of model predictions and conservation decisions (Araújo et al. 2019), where relevant and possible, describe how you will characterise and report the spatial, temporal and environmental pattern of errors and variance. If comparing alternative models, specify what measures of model comparison or out-of-sample performance metrics will you use to find support for alternative models or else to optimise predictive ability. State what numerical threshold or qualities you will use for each of these metrics."
  },
  {
    "objectID": "preregistration_modelling.html#model-analysis",
    "href": "preregistration_modelling.html#model-analysis",
    "title": "Adaptive Preregistration for Modelling",
    "section": "5.3 Model analysis",
    "text": "5.3 Model analysis\n\n\n\n\n\n\nRationale & Explanation\n\n\n\nUncertainty in models arises due to incomplete system understanding (which processes to include, or which interact), from imprecise, finite and sparese data measurements, and from uncertainty in input conditions and scenarios for model simulations or runs (Jakeman, Letcher, and Norton 2006). Non-technical uncertainties can also be introduced throughout the modellign process, such as uncertainties arising from issues in problem-framing, indeterminicies, and modeller / client values (Jakeman, Letcher, and Norton 2006).\nThe purpose of model analysis is to prevent blind trust in the model by understanding how model outputs have emerged, and to ‘challenge’ the model by verifying whether the model is still believable and fit for purpose if one or more parameters are changed (Grimm et al. 2014).\nModel analysis should increase understanding of the model behaviour by identifying which processes and process interactions explain characteristic behaviours of the model system. Model analysis typically consists of sensitivity analyses preceded by uncertainty analyses (Saltelli et al. 2019), and a suite of other simulation or other computational experiments. The aim of such computational experiments is to increase understanding of the model behaviour by identifying which processes and process interactions explain characteristic behaviours of the model system (Grimm et al. 2014). Uncertainty analyses and sensitivity analyses augment one another to draw conclusions about model uncertainty.\nBecause the results from a full suite of sensitivity analysis and uncertainty analysis can be difficult to interpret due to the number and complexity of causal relations examined (Jakeman, Letcher, and Norton 2006), it is useful for the analyst to relate the choice of analysis to the modelling context, purpose and analytical objectives defined in the problem formulation phase, in tandem with any critical uncertainties that have emerged during model development and testing prior to this point.\n\n\n\n5.3.1 Uncertainty Analyses\n\n\n\n\n\n\nExplanation\n\n\n\nUncertainty can arise from different modelling techniques, response data and predictor variables (Araújo et al. 2019). Uncertainty analyses characterise the uncertainty in model outputs, and identify how uncertainty in model parameters affects uncertainty in model output, but does not identify which model assumptions are driving this behaviour (Grimm et al. 2014; Saltelli et al. 2019). Uncertainty analyses can include propagating known uncertainties through the model, or by investigating the effect of different model scenarios with different parameters and modelling technique combinations (Araújo et al. 2019), for example. It could also include characterising the output distribution, such as through empirical construction using model output data points. It could also include extracting summary statistics like the mean, median and variance from this distribution, and perhaps constructing confidence intervals on the mean (Saltelli et al. 2019).\n\n\n\n\n\n\n\n\nPreregistration Item\n\n\n\n\nPlease describe how you will characterise model and data uncertainties, e.g. propagating known uncertainties through the model, investigating the effect of different model scenarios with different parameters and modelling technique combinations (Araújo et al. 2019), or empirically constructing model distributions from model output data points, and extracting summary statistics, including the mean, median, variance, and constructing confidence intervals (Saltelli et al. 2019).\nRelate your choice of analysis to the context and purposes of the model described in the problem formulation phase. For instance ‚discrepancies between model output and observed output may be important for forecasting models, where cost, benefit, an risk over a substantial period must be gauged, but much less critical for decision-making or management models where the user may be satisfied with knowing that the predicted ranking order of impacts of alternative scenarios or management options is likely to be correct, with only a rough indication of their sizes” (Jakeman, Letcher, and Norton 2006).\nBriefly describe how you will summarise the results of these in silico experiments with graphical, tabular, or other devices, such as summary statistics.\nIf the chosen modelling approach is able to explicitly articulate uncertainty due to data, measurements or baseline conditions, such as by providing estimates of uncertainty (typically in the form of probabilistic parameter covariance, (Jakeman, Letcher, and Norton 2006)), specify which measure of uncertainty you will use.\n\n\n\n\n\n5.3.2 Sensitivity Analyses\n\n\n\n\n\n\nExplanation\n\n\n\nSensitivity analysis examines how uncertainty in model outputs can be apportioned to different sources of uncertainty in model input (Saltelli et al. 2019).\n\n\n\n\n\n\n\n\nPreregistration Item\n\n\n\n\nDescribe the sensitivity analysis approach you will take: deterministic sensitivity, stochastic sensitivity (variability in the model), or scenario sensitivity (effect of changes based on scenarios).\nDescribe any sensitivity analyses you will conduct by specifying which parameters will be held constant, which will be varied, and the range and intervals of values over which those parameters will be varied.\nState the primary objective of each sensitivity analysis, for example, to identify which input variables contribute the most to model uncertainty so that these variables can be targeted for further data collection, or alternatively to identify which variables or factors contribute little to overall model outputs, and so can be ‘dropped’ from future iterations of the model (Saltelli et al. 2019).\n\n\n\n\n\n5.3.3 Model application or scenario analysis\n\n\n\n\n\n\nPreregistration Item\n\n\n\n\nSpecify any input conditions and relevant parameter values for initial environmental conditions and decision-variables under each scenario specified in section 1.\nDescribe any other relevant technical details of model application, such as methods for how you will implement any simulations or model projections.\nWhat raw and transformed model outputs will you extract from the model simulations or projections, and how will you map, plot, or otherwise display and synthesise the results of scenario and model analyses.\nExplain how you will analyse the outputs to answer your analytical objectives. For instance, describe any trade-off or robustness analyses you will undertake to help evaluate and choose between different alternatives in consultation with experts or decision-makers.\n\n\n\n\n\n5.3.4 Other simulation experiments / robustness analyses\n\n\n\n\n\n\nPreregistration Item\n\n\n\n\nDescribe any other simulation experiments, robustness analyses or other analyses you will perform on the model, including any metrics and their criteria / thresholds for interpreting the results of the analysis."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Adaptive Preregistration for Model-Based Research",
    "section": "",
    "text": "This site provides a guide to implementing Adaptive Preregistration for Model-Based Research, with a focus on ecological modelling as it is used in both basic and applied contexts. This site covers the following topics:\n\nBackground on researcher degrees of freedom, the risk of questionable research practices (QRPs), and introduction to preregistration as a tool for mitigating QRPs.\nAdaptive Preregistration\n\nBackground: What is adaptive preregistration, why do we need it for registering modeling analyses, and what are its defining features?\nA practical guide: How to implement adaptive preregistration using git and GitHub.\n\nPreregistration template for model-based research in ecology and conservation.\nTBC An overview of the sorts of things that we should preregister when modelling, what practices to look out for, what things should be registered as decision-trees.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "RDF_QRP.html",
    "href": "RDF_QRP.html",
    "title": "From Researcher Degrees of Freedom to Questionable Research Practices",
    "section": "",
    "text": "Throughout the course of model development and analysis researchers make numerous decisions. These alternative decisions are termed ‘researcher degrees of freedom’ (Wicherts et al. 2016; Simmons, Nelson, and Simonsohn 2011). These decisions are usually subjective and arbitrary yet methodologically and substantively acceptable (Wicherts et al. 2016), however may collectively significantly influence the final ‘analysis strategy’ (sensu Hoffmann et al. 2021) or ‘specification’ (sensu Simonsohn, Simmons, and Nelson 2020), and consequently the research findings and conclusions drawn from that analysis (Desbureaux 2021).\nAlternative decisions about model assumptions and implementation can lead researchers down very different analytical paths that result in ‘parallel’ or ‘adjacent’ worlds with different interpretations, and potentially conflicting conclusions originating from a single underlying dataset (Young and Stewart 2021; Liu, Althoff, and Heer 2020). The set of all (plausible or reasonable) possible decisions that a researcher could make over the entire analysis from beginning to end has been termed collectively as the ‘garden of forking paths’ (Gelman and Loken 2013) or the ‘multiverse’, which consists of multiple universes of plausible analysis strategies (Steegen et al. 2016).\nResearcher degrees of freedom are not inherently problematic, but rather are a product of the uncertainty that besets model development and analysis, whether due to incomplete theoretical understanding of underlying ecological processes, or because modelling is an act of simplification and abstraction; there is always some degree of choice about which ecological processes and phenomena should be included in the model, how they should be represented (Babel, Vinck, and Karssenberg 2019; Getz et al. 2017). Further uncertainty and multiplicity of analysis strategies stems from uncertainty in data preprocessing, model, parameter, and method uncertainty (Hoffmann et al. 2021)."
  },
  {
    "objectID": "RDF_QRP.html#researcher-degrees-of-freedom",
    "href": "RDF_QRP.html#researcher-degrees-of-freedom",
    "title": "From Researcher Degrees of Freedom to Questionable Research Practices",
    "section": "",
    "text": "Throughout the course of model development and analysis researchers make numerous decisions. These alternative decisions are termed ‘researcher degrees of freedom’ (Wicherts et al. 2016; Simmons, Nelson, and Simonsohn 2011). These decisions are usually subjective and arbitrary yet methodologically and substantively acceptable (Wicherts et al. 2016), however may collectively significantly influence the final ‘analysis strategy’ (sensu Hoffmann et al. 2021) or ‘specification’ (sensu Simonsohn, Simmons, and Nelson 2020), and consequently the research findings and conclusions drawn from that analysis (Desbureaux 2021).\nAlternative decisions about model assumptions and implementation can lead researchers down very different analytical paths that result in ‘parallel’ or ‘adjacent’ worlds with different interpretations, and potentially conflicting conclusions originating from a single underlying dataset (Young and Stewart 2021; Liu, Althoff, and Heer 2020). The set of all (plausible or reasonable) possible decisions that a researcher could make over the entire analysis from beginning to end has been termed collectively as the ‘garden of forking paths’ (Gelman and Loken 2013) or the ‘multiverse’, which consists of multiple universes of plausible analysis strategies (Steegen et al. 2016).\nResearcher degrees of freedom are not inherently problematic, but rather are a product of the uncertainty that besets model development and analysis, whether due to incomplete theoretical understanding of underlying ecological processes, or because modelling is an act of simplification and abstraction; there is always some degree of choice about which ecological processes and phenomena should be included in the model, how they should be represented (Babel, Vinck, and Karssenberg 2019; Getz et al. 2017). Further uncertainty and multiplicity of analysis strategies stems from uncertainty in data preprocessing, model, parameter, and method uncertainty (Hoffmann et al. 2021)."
  },
  {
    "objectID": "RDF_QRP.html#questionable-research-practices",
    "href": "RDF_QRP.html#questionable-research-practices",
    "title": "From Researcher Degrees of Freedom to Questionable Research Practices",
    "section": "Questionable Research Practices",
    "text": "Questionable Research Practices\nWhen researchers are faced with ambiguous decisions where there is no single decision or strategy acceptable according to scientific standards and consensus, we are likely to select the decision that results in the most ‘favourable’ finding with “convincing self-justification” (Simmons, Nelson, and Simonsohn 2011; Hoffmann et al. 2021). Researchers routinely execute and explore alternative analysis pathways in service of identifying the most favourable (i.e. publishable) result, selectively reporting only one or several preferred results (Liu, Althoff, and Heer 2020; Simmons, Nelson, and Simonsohn 2011).\nImportantly, as Gelman and Loken note (Gelman and Loken 2013) this behaviour is not a consciously nefarious ‘fishing expedition’, rather, because researchers are motivated actors in a ‘publish or perish’ culture who are susceptible to cognitive biases and are willing to change their modelling assumptions when the results conflict with desired outcomes – sincerely and self-convincingly believing that the more favourable model is superior (Young and Stewart 2021).\nThus researcher degrees of freedom precipitate questionable research practices when they remain both undisclosed and exploited opportunistically (Bakker et al. 2018). Undisclosed and opportunistic use of researcher degrees of freedom artificially increases the probability of false positive results, inflates effect size estimates and ultimately reduces the replicability of published findings while encouraging overconfidence in the precision of results and exascerbating the risk of accepting and propagating false facts (Wicherts et al. 2016; Hoffmann et al. 2021; Roettger 2019; Nissen et al. 2016). Emerging evidence suggests that rates of QRPs in ecology and evolutionary biology are similar to rates observed in other disciplines beset by reproducibility crises (Fraser et al. 2018)."
  },
  {
    "objectID": "RDF_QRP.html#preregistration-mitigates-against-researcher-degrees-of-freedom",
    "href": "RDF_QRP.html#preregistration-mitigates-against-researcher-degrees-of-freedom",
    "title": "From Researcher Degrees of Freedom to Questionable Research Practices",
    "section": "Preregistration mitigates against researcher degrees of freedom",
    "text": "Preregistration mitigates against researcher degrees of freedom\nPreregistration is an open-science practice adopted in scientific disciplines that have begun to confront the ‘reproducibility crisis’, and aims to improve research transparency and mitigate questionable research practices within a study by distinguishing between what is a genuine a priori planned analysis, and what is not (Parker, Fraser, and Nakagawa 2019). Preregistration requires the methods and analysis plan of a future study are registered in a secure and publicly accessible platform prior to any data collection, and after submission it can no longer be altered.\nDespite enthusiastic uptake of preregistration in some scientific disciplines, there has been little uptake of pre-registration in ecology and conservation, and many modellers and researchers engaged in non-hypothesis testing research both within and outside of ecology have eschewed preregistration on the grounds that preregistration and existing templates are irrelevant because they are focused on null-hypothesis significance testing (e.g. McDermott et al. 2021; Prosperi et al. 2019).\nInstead of throwing out the baby with the bathwater, this guide is premised on the idea that preregistration can and should be used in model-based research in ecology and conservation, but its particular form should reflect the norms and practice of the research context in which it is applied, in order to adequately restrict researcher degrees of freedom that may uniquely emerge within a domain-specific methodology. The work presented in this guide aims to translate preregistration into model-based research within ecology and conservation by proposing an expanded view of preregistration we term Adaptive Preregistration. We also present a template for preregistering model-based research in ecology and conservation."
  },
  {
    "objectID": "adaptive_preregistration_background.html",
    "href": "adaptive_preregistration_background.html",
    "title": "A primer on Adaptive Preregistration",
    "section": "",
    "text": "Figure 1: Conventional preregistration process for registered reports, as illustrated by (Pu et al. 2019).\n\n\n\n\nThe iterative nature of model development is a significant barrier to adopting preregistration for model-based research. Translating the preregistration process to a modelling research context is challenging because the iterative nature of model development conflicts with the inner logic of existing preregistration templates that presume a linear research workflow. The process of preregistration as it is currently implemented is tailored towards the hypothetico-deductive model of the scientific method. As such, the process of preregistration is completely distinct from, and precedes the implementation of the analysis plan laid out in the preregistration – analytic decisions are completely independent from the data.\nIn fact, preregistration has been defined as “the action of confirming an unalterable version of one’s research plan prior to collecting data” (Pu et al. 2019) or prior to analysing the data (Mertens and Krypotos 2019). In this process, researcher sequentially shifts from ideation, to study/analysis design and preregistration, to collecting and analysing the data, writing the report or manuscript, to documenting the research and publishing (Figure 1). The iterative nature of model development, however, inherently precludes this model of preregistration from being applied to model-based research in ecology and conservation."
  },
  {
    "objectID": "adaptive_preregistration_background.html#background-motivation",
    "href": "adaptive_preregistration_background.html#background-motivation",
    "title": "A primer on Adaptive Preregistration",
    "section": "",
    "text": "Figure 1: Conventional preregistration process for registered reports, as illustrated by (Pu et al. 2019).\n\n\n\n\nThe iterative nature of model development is a significant barrier to adopting preregistration for model-based research. Translating the preregistration process to a modelling research context is challenging because the iterative nature of model development conflicts with the inner logic of existing preregistration templates that presume a linear research workflow. The process of preregistration as it is currently implemented is tailored towards the hypothetico-deductive model of the scientific method. As such, the process of preregistration is completely distinct from, and precedes the implementation of the analysis plan laid out in the preregistration – analytic decisions are completely independent from the data.\nIn fact, preregistration has been defined as “the action of confirming an unalterable version of one’s research plan prior to collecting data” (Pu et al. 2019) or prior to analysing the data (Mertens and Krypotos 2019). In this process, researcher sequentially shifts from ideation, to study/analysis design and preregistration, to collecting and analysing the data, writing the report or manuscript, to documenting the research and publishing (Figure 1). The iterative nature of model development, however, inherently precludes this model of preregistration from being applied to model-based research in ecology and conservation."
  },
  {
    "objectID": "adaptive_preregistration_background.html#iterative-cycle-of-model-development-and-preregistration",
    "href": "adaptive_preregistration_background.html#iterative-cycle-of-model-development-and-preregistration",
    "title": "A primer on Adaptive Preregistration",
    "section": "Iterative Cycle of Model Development and Preregistration",
    "text": "Iterative Cycle of Model Development and Preregistration\nCurrent preregistration practices based on NHST-focussed accounts of questionable research parctices prescribe the principle of ‘decision-independence’, where analysis decisions are made independently of and a priori to analysing the data (Srivastava 2018; Gelman and Loken 2013). However, the modelling process may genuinely violate the principle of ‘decision-independence’ because it is non-linear, iterative and usually generates many interim versions of the model preceding publication. During modelling a researcher will almost always engage in state-dependent decision-making — that is, decisions about whether to alter the model in a subsequent round of model development depend on the results of model testing, evaluation and analysis at the end of the previous iteration of model development. These practices aren’t necessarily questionable in and of themselves, but do have the potential to become questionable if they are exploited in service of artificially inflating the accuracy or precision of the model, its predictions or evaluation tests to the effect that the model is perceived to be more credible than it would be if that practice did not occur – essentially data-dependent decisions that lead to consumers of the model placing unsubstantiated belief in the reliability, validity and utility of the model and its outputs are questionable (Gould et al., n.d.) and can be mitigated through the practice of preregistration.\nVery often in modelling, there are decisions that are necessarily dependent on the outcome of previous analytic decisions in the modelling workflow (Liu, Althoff, and Heer 2020). Firstly, preliminary or investigatory analyses might need to be conducted before being able to specify future decision-steps in the analysis plan. For example, modellers might need to check the distribution of particular variables in order to determine how to specify the model, perform assumption checks, or check for collinearity or spatial autocorrelation. Secondly, data-dependent decisions from model checking may justifiably result in changes to either the analysis or to interpretations of the mode (Srivastava 2018). For instance, inspection of the residuals and other model checking tests may force the modeller to return to earlier decision-points and change the planned analysis or model. Other times, the planned analysis and specified model may simply not converge, or the model is saturated and runs out of variation to apportion such that the model and or the model fitting algorithm must be respecified and re-implemented. Other times, the modelling process itself may generate knowledge and learning about the system, and the model structure changes throughout the process of model development.\nFinally, sometimes there are some decisions that are too difficult to anticipate, or simply cannot be made in advance (Srivastava 2018). This is particularly true for decisions occurring at the later phases of the modelling construction and development process (Figure 2, TBC), where downstream decisions depend on the outcomes of earlier decisions and outcomes of modelling analyses. Some decisions might not be able to be specified on the first attempt at writing the preregistration until the model is fully or at least close to fully specified, for example, specifying precisely what and how sensitivity analyses or uncertainty analyses will be conducted.\nAs illustrated above, the adaptive nature of analysis during the model development process contravenes the central principle of preregistration – decision-independence and is fundamentally incompatible with the sequential, linear process of traditional preregistration where the analysis plan is fully specified prior to observing and analysing the data."
  },
  {
    "objectID": "adaptive_preregistration_background.html#adaptive-preregistration",
    "href": "adaptive_preregistration_background.html#adaptive-preregistration",
    "title": "A primer on Adaptive Preregistration",
    "section": "Adaptive Preregistration",
    "text": "Adaptive Preregistration\nIn order to address the disjuncture between the iterative nature of model development, and the single-use, deterministic preregistration template format, we take an ‘expanded view of preregistration’, that fits with the concept of ‘adaptive preregistration’ proposed by Srivastava (2018). The modeller proceeds through the model development process, they will switch they will shift from ideation, preregistration, execution of the analysis and back to ideation and preregistration again, implementing implementing parts of the analysis plan that were specified in the previous preregistration step, and letting the outcomes in that analysis step inform the specification of the next analysis ( Figure 4).\nThis view of preregistration breaks with the current model of preregistration, wherein the author writes a single deterministic preregistration containing a rule for every decision, and sequentially shifts from ideation, preregistration to execution of the plan. Two features define adaptive preregistration:\n\nIt contains ‘plans to deploy flexible strategies’ (Srivastava 2018) wherein the author can supply heuristic consisting of multiple different analysis or modelling strategies whose execution depends on the outcome of previous decision-points or analyses, for example, decision-trees could be preregistered prior to analysing the data (Baldwin et al. 2022).\nThe preregistration may be iterative and consist of interim preregistrations that mark phases of modelling and analysis as different parts of the data are observed (Srivastava 2018).\n\n\nPreregistering Flexibility\nWhen preliminary analyses, model checking or other data-dependent decisions need to be made, the modeller can specify a ‘flexible strategy’ within their preregistration by:\n\nstating what quantity or outcome needs to be known to move forward with the modelling and analysis,\nexplaining what test or analysis will be performed to obtain this quantity or outcome, and what parts of the data will be used in this analysis, and\ndescribing how the results will be interpreted, listing each potential decision and its trigger, where possible.\n\n\n\n\n\n\nFigure 2: A general heuristic in the form of a decision-tree for registering flexibile analyses.\n\n\n\n\n\n\nInterim Preregistrations\nThe modeller follows an iterative process of preregistration. As they proceed through the model development process, they can shift from ideation, preregistration and execution of the analysis plan, to preregistration again. Depending on observed outcomes of the pre-specified decision-trees or registered flexible heuristics, interim preregistrations are created at multiple points in the model development process, each time there is an addition or amendment to the analysis plan.\n\n\n\n\n\nFigure 3: Adaptive preregistration can cycle through the specification, execution and ideation phases multiple times in adaptive preregistration, with interim versions of the preregistration being generated at each preregistration point."
  },
  {
    "objectID": "adaptive_preregistration_background.html#documenting-adaptive-preregistration-transparently",
    "href": "adaptive_preregistration_background.html#documenting-adaptive-preregistration-transparently",
    "title": "A primer on Adaptive Preregistration",
    "section": "Documenting adaptive preregistration transparently",
    "text": "Documenting adaptive preregistration transparently\n\nThe Checking Problem\nFollowing publication of the final study, authors, reviewers, editors and readers may all engage in the process of checking a manuscript or published paper against its preregistration to verify that the study and analyses were conducted as specified in the preregistration. The process of preregistration is already cumbersome, because the dominant format of the preregistration is a single, static text-based document designed with the study authors in mind to very quickly insert the required analysis plan information, rather than to facilitate a comparison of the reported analysis and results against the planned analysis in the preregistration. In their current form, they are almost exclusively exist as “write-only media” (Pu et al. 2019). The task of model checking is further complicated by the non-linear and iterative nature of model development, and by our proposed adaptive preregistration methodology, because in most modelling studies there will likely be interim preregistrations, such that there are multiple versions of the preregistration to check against the completed analysis."
  },
  {
    "objectID": "adaptive_preregistration_background.html#implementing-adaptive-preregistration-with-git-and-github",
    "href": "adaptive_preregistration_background.html#implementing-adaptive-preregistration-with-git-and-github",
    "title": "A primer on Adaptive Preregistration",
    "section": "Implementing Adaptive Preregistration with git and GitHub",
    "text": "Implementing Adaptive Preregistration with git and GitHub\nGiven the iterative nature of adaptive preregistration then, the current convention of the single-use, deterministic and static text-based document format for preregistration is inadequate. Interim versions of a preregistration must be time-stamped and ideally version-controlled, such that it is clear not just that two or more versions differ from each other, and in what order they were created, but so that it is explicitly clear in how and why they differ. To facilitate preregistration checking of flexible modelling and analysis strategies, we must link the results of both final and preliminary analyses back to the specified strategy: the link between a particular triggered analysis decision, and the analysis outcome that triggered the decision must be explicit. Additionally, if the results of some preliminary analyses cause revisions to the model itself or a different modelling procedure or approach to be selected than was originally or previously planned, then the trigger for this decision must be explicitly linked to the next version of the preregistration.\nWe propose leveraging the features of git and GitHub to facilitate implementation both adaptive preregistration in a transparently documented manner (Figure 4), and provide a step-by-step guide.\n\n\n\n\n\nFigure 4: Adaptive Preregistration for model-based research using GitHub."
  },
  {
    "objectID": "adaptive_preregistration_implementation.html",
    "href": "adaptive_preregistration_implementation.html",
    "title": "Implementing Adaptive Preregistration with Git and Github",
    "section": "",
    "text": "We propose leveraging both the version-control and the project management features of GitHub (www.github.com) as a tool for implementing adaptive preregistration. GitHub is a web-interface that utilises the version-control system, ‘git’, to store files, track changes, and enable collaboration on computer code (Braga et al. 2023). When a document is version-controlled using Git and GitHub, each version of a document can be time-stamped, and is assigned its own unique identifier, or commit-hash1 so that chronological records of changes to a collection of files, stored in a ‘repository’, are made transparent (Braga et al. 2023; Bryan and Crossman 2008).\nWhy GitHub for Adaptive Preregistration?\n\nBecause GitHub documents are ‘time-stamped’ the genesis of the preregistration from one version to the next is apparent.\nMoreover, using the diff view of a document on GitHub, exactly how and where a document has changed between versions is clear - additions are coloured green, deletions are coloured red, and changes within a line are highlighted.\nThe project management and communication features of GitHub, such as GitHub Issues,\ntagging and release\n\nGitHub users can collaborate by contributing, modifying and discussing existing code, reporting bugs and other problems, discover new code and data, and publish new code.\nGitHub also integrates several communication features, such as Github Issues, Github Discussions, Github Pages, which allows users to engage in discussions, to plan and collaborate on code, and publish information to a webpage (see Box 1).” (Braga et al., 2023, p. 1365) (pdf)\n\n\n\n\n\nFigure 1: ‘Diff View’ of changes made to a preregistration in process.\n\n\n\n\nThe preregistration document should be saved within the project repository that contains the data and modelling and analysis code, such that any results from analyses that cause the analysis plan to change can be linked and referenced in the updated version of the plan – thus facilitating the process of model checking. By providing a method for contrasting the actual analysis undertaken against the preregistered analysis, this provides a mechanism for allowing reviewers to properly evaluate the preregistration against the reported analysis. It thus provides a way of marking preregistered parts of a report from non-preregistered parts of the analysis."
  },
  {
    "objectID": "adaptive_preregistration_implementation.html#adaptive-preregistration-using-github",
    "href": "adaptive_preregistration_implementation.html#adaptive-preregistration-using-github",
    "title": "Implementing Adaptive Preregistration with Git and Github",
    "section": "",
    "text": "We propose leveraging both the version-control and the project management features of GitHub (www.github.com) as a tool for implementing adaptive preregistration. GitHub is a web-interface that utilises the version-control system, ‘git’, to store files, track changes, and enable collaboration on computer code (Braga et al. 2023). When a document is version-controlled using Git and GitHub, each version of a document can be time-stamped, and is assigned its own unique identifier, or commit-hash1 so that chronological records of changes to a collection of files, stored in a ‘repository’, are made transparent (Braga et al. 2023; Bryan and Crossman 2008).\nWhy GitHub for Adaptive Preregistration?\n\nBecause GitHub documents are ‘time-stamped’ the genesis of the preregistration from one version to the next is apparent.\nMoreover, using the diff view of a document on GitHub, exactly how and where a document has changed between versions is clear - additions are coloured green, deletions are coloured red, and changes within a line are highlighted.\nThe project management and communication features of GitHub, such as GitHub Issues,\ntagging and release\n\nGitHub users can collaborate by contributing, modifying and discussing existing code, reporting bugs and other problems, discover new code and data, and publish new code.\nGitHub also integrates several communication features, such as Github Issues, Github Discussions, Github Pages, which allows users to engage in discussions, to plan and collaborate on code, and publish information to a webpage (see Box 1).” (Braga et al., 2023, p. 1365) (pdf)\n\n\n\n\n\nFigure 1: ‘Diff View’ of changes made to a preregistration in process.\n\n\n\n\nThe preregistration document should be saved within the project repository that contains the data and modelling and analysis code, such that any results from analyses that cause the analysis plan to change can be linked and referenced in the updated version of the plan – thus facilitating the process of model checking. By providing a method for contrasting the actual analysis undertaken against the preregistered analysis, this provides a mechanism for allowing reviewers to properly evaluate the preregistration against the reported analysis. It thus provides a way of marking preregistered parts of a report from non-preregistered parts of the analysis."
  },
  {
    "objectID": "adaptive_preregistration_implementation.html#how-it-works",
    "href": "adaptive_preregistration_implementation.html#how-it-works",
    "title": "Implementing Adaptive Preregistration with Git and Github",
    "section": "How it works…",
    "text": "How it works…\nGitHub is used to track amendments to the preregistration — each time an additional template item is answered, or an alteration is made to the preregistration, the change should be committed to the repository. Semantic versioning is used to document each version of the preregistration. Semantic versioning2 is an open-source software engineering practice used to document and and communicate the development of software, in a way that allows others to depend on it (Kitzes, Turek, and Deniz 2018). We have created our own semantic versioning scheme to be used to specifically document the incremental version of the preregistration, in tandem with GitHub’s tag and release feature, each interim preregistration is tagged and released with its semantic version number. All template items that can be completed should be completed before any data analysis or modelling proceeds. The preregistration should be completed until a) no further detail can be supplied or b) no additional template item can be answered until preliminary investigations or analyses are undertaken (Fig 3, steps 1 - 3).\n\n\n\n\n\nFigure 2: Initial Preregistration: All template items that can be answered should be completed prior to data analysis or modelling.\n\n\n\n\n\n…Shifting from planning to execution…\n\n\n\n\n\nFigure 3: Document: After the initial preregistration, document the initial preregistration using GitHub issues.\n\n\n\n\nBefore shifting from planning to the execution of the interim preregistration, each complete response in the interim preregistration should have a corresponding GitHub issue3 created (Figure 3, step 3, ‘Document’). Relevant discussion between analysts about how to proceed with the intended analysis or interpreting preliminary results are tracked within an issue’s thread. Each issue is automatically assigned a unique number by GitHub. Any code or analysis outputs, such as figures, tables or other files, should be committed and tagged with the issue number, so that all analysis addressing that task is tracked within the issue’s thread (Fig 3, step 4, ‘Do’). The URL for the GitHub issue should then be added to the relevant preregistration item, the preregistration document should then be committed with the changes, and the version number of the preregistration updated in a new release.\n\n\n… And back again.\n\n\n\n\n\nFigure 4: Adaptive Preregistration - When Plans Change\n\n\n\n\nAs the modelling proceeds, further detail on later phases of the analysis can be iteratively updated and preregistered. Where the results of model evaluation and analysis reveal that there are problems with the model, plans can be changed, and again the next phase can be preregistered. For example, if a researcher finds that assumptions are violated or other unexpected results occur that force a change to the planned analysis, the preregistration can be updated based on the findings of those analyses. When this occurs, the major version number of the preregistration document should be incremented (Fig 3 steps 5 - 6). Since the findings of the analyses are tracked within the relevant issue for that template item, and the issue is recorded in the preregistration itself, the trigger for the change in the plan is made explicitly clear.\nFor each altered or new preregistration item, the issue thread should either be updated, or a new issue created respectively (Fig 3, step 6). Those analyses are then either reconnected based on the revised plan or the new investigatory analyses are then conducted (Fig 3, step 7).\nThis process continues until the preregistration is complete, and the researcher can continue to execute the plan as it has been fully described in the final version of the preregistration document (Fig 3, step 8 - 9).\n\n\nFlexible Strategies, Preliminary Analyses or Investigations\nWhen a researcher needs to conduct preliminary analyses or check parts of the data before committing to a particular decision about the model or analysis, they can specify a flexible strategy in their response to that preregistration item by:\n\nstating what needs to be known to move forward with the modelling and analysis and why the analysis is necessary\nexplaining how the researcher will test this, and what parts of the data will be used in that analysis\ndescribing how the results will be interpreted, listing each potential decision and the analysis result that will trigger that decision, where possible."
  },
  {
    "objectID": "adaptive_preregistration_implementation.html#the-initial-preregistration-phase",
    "href": "adaptive_preregistration_implementation.html#the-initial-preregistration-phase",
    "title": "Implementing Adaptive Preregistration with Git and Github",
    "section": "The initial preregistration phase",
    "text": "The initial preregistration phase\n\nThe first iteration of preregistration\n\nEdit the document with your responses to the template items\n\n\nView the document in GitHub here\nClick on The ‘edit document’ button: \nYou will be able to edit the document directly in GitHub, Edit away!\n\n\nCommit changes\n\n\nIf you are finished, or ready to save your changes by ‘committing’ them, commit by scrolling underneath the document editing window. There will be a box called “commit changes”, enter in the commit message box the following:\nYour message should contain the following: Initial Preregistration #IssueNumber. Tag the Preregistration GitHub issue number in the commit message (For this repository, it’s #17).\nFill out the additional description box briefly describing the changes you just made\nClick “commit”\n\n\n\n\nScreen Shot 2020-05-18 at 12 19 29 pm"
  },
  {
    "objectID": "adaptive_preregistration_implementation.html#incrementally-updating-the-preregistration-plan",
    "href": "adaptive_preregistration_implementation.html#incrementally-updating-the-preregistration-plan",
    "title": "Implementing Adaptive Preregistration with Git and Github",
    "section": "Incrementally Updating the Preregistration Plan",
    "text": "Incrementally Updating the Preregistration Plan\n\nMoving from general to specific: Incrementally updating the preregistration\nYou will likely find that some items on the template cannot be answered just yet. One reason being that the level of detail required to adequately answer the template item just isn’t clear yet, and more thought and time is needed, or you need to be further along the modelling process to be able to adequately answer the question. Another reason is that downstream decisions might depend on some preliminary investigations of the data – for example, you might need to explore the shape of a measured variable before you can properly specify the full model.\nIn the latter situation, where possible, you can preregister your preliminary analysis: describing what the preliminary analysis entails, providing a short and relevant list of plausible outcomes, and describing for each of those possible outcomes how the dependent or down-stream decision-point will change depending on the outcome.\n\n\nUsing GitHub to update the plan\nWe will use GitHub to track amendments to the preregistration for the two situations described above ( a) adding more detail, answering incomplete questions; b) Responding to findings of a preliminary analysis already described in your preregistration).\nTo make the changes to the preregistration document, follow the same instructions as for creating the initial preregistration above - simply edit the preregistration in GitHub, and commit the changes.\nAs for the situation where you are responding to findings of a preliminary analysis that you have already described in your preregistration, see the below sections on Moving from planning to doing and Shifting back to planning from doing."
  },
  {
    "objectID": "adaptive_preregistration_implementation.html#moving-from-planning-to-doing",
    "href": "adaptive_preregistration_implementation.html#moving-from-planning-to-doing",
    "title": "Implementing Adaptive Preregistration with Git and Github",
    "section": "Moving from planning to doing",
    "text": "Moving from planning to doing\nBefore you begin to do or start anything you’ve planned within your preregistration, you must create a GitHub issue for that item, phase, or step in the modelling process, follow these steps:\n\nView the preregistration document in Github, and click on the line-numbers on the left to highlight the PRT item text, as well as your response to the item. For example, let’s say you want to start working the model checking, you would highlight the following in the document. Note that once you’ve highlighted some lines, a little box with an ellipsis appears on top of the lines numbers. \nClick on the tiny box with the ellipsis, and a pop-up dialogue box will appear with 4 options: Copy lines, Copy permalink, View git blame, References in new issue. Select the last option Reference in new issue. \nThis will open up a new window with a new GitHub Issue for you to complete. Note that part of the issue description has already been filled out for you, this is a reference to the lines you just highlighted:  Which is cool, because if you open the Preview tab to view the preview of your issue, it will look like this: \n\nAdd any further information or technical details about going forward and working on this task, as necessary. Submit the issue to save. Any work, and subsequent commits on this topic should be tagged with the issue number. We now have a mechanism for linking your actual analysis and actions to the planned analysis in the preregistration document.\n\nDealing with unexpected outcomes, and changing plans: Shifting back to planning from doing\nSometimes assumptions are violated, or there are other unexpected outcomes of preliminary analyses, and you will need to change plans. How do we deal with this?\n\nAdd the URL to the relevant GitHub issue under the corresponding Preregistration Item.\nAdd any the URL for any linked commits to the Preregistration Item.\nrevise the Preregistration, where necessary by editing the Preregistration document in GitHub, as per previous instructions.\nCommit the changes with the following commit message header:\nDescribe the trigger for the Preregistration revisions in brief detail within the commit message body, adding the link to the relevant GitHub issue containing the tagged commits that triggered the revision."
  },
  {
    "objectID": "adaptive_preregistration_implementation.html#living-preregistration---tracking-changes-to-the-preregistration-draft",
    "href": "adaptive_preregistration_implementation.html#living-preregistration---tracking-changes-to-the-preregistration-draft",
    "title": "Implementing Adaptive Preregistration with Git and Github",
    "section": "‘Living preregistration’ - tracking changes to the Preregistration Draft",
    "text": "‘Living preregistration’ - tracking changes to the Preregistration Draft\nWe will leverage GitHub’s tagging and release feature in conjunction with Semantic Versioning to document and track changes to the preregistration document.\nGitHub releases mark software iterations that you can release and deploy4. Within the context of adaptive preregistration, we can use releases to mark interim versions of the preregistration, as well as to take a snapshot of the entire repository at that moment in time.\nNote that anything up until the complete preregistration should be additionally tagged as a ‘pre-release’ to communicate that the initial preregistration is not yet complete.\n\n\nTable 1: Types of changes to the preregistration and corresponding actions and versioning adjustments\n\n\n\n\n\n\n\nPreregistration Change or Action\nGitHub Mechanism\nSemantic Versioning\n\n\n\n\nChanges preceding initial completion of preregistration\nTag and Mark as ‘pre-release’\nMinor increment. E.g. 0.2.0 The Major value should not exceed 0.\n\n\nIncremental Change, e.g. respond to new question.\nTag and Release\nMinor increment. E.g. 0.2.0 The Major value should not exceed 0.\n\n\nInitial Preregistration complete. Note that ‘complete’ needs to be decided. Is complete after all preliminary and investigatory analyses have been done and the final Preregistration is complete, or is it the first iteration, where no more items can be filled out until some initial analyses are undertaken?\nTag and Release\nMajor increment to 1.0.0\n\n\nMajor change to a complete Preregistration – no looking or analysing at data\nTag and Release\nMajor increment &gt;1.0.0\n\n\nLinking implemented work to the Preregistration Item\nUpdate Preregistration with GH Issue Link and all relevant commit hashes pertaining to that Preregistration Item + Commit. Tag and Release that preregistration commit.\nMinor increment.\n\n\nData-dependent changes with registered flexibility Adapting the Preregistration on the basis of results from preliminary or investigatory analyses\nUpdate Preregistration with GH Issue Link and all relevant commit hashes pertaining to that Preregistration Item + Commit. Respond to dependent incomplete Preregistration Items or Update existing / already completed Preregistration Items + Commit. Tag and Release\nMajor increment.\n\n\nMinor mistakes\nChanges including small typos, or minor elaborations on existing specified plans\nPatch increment. E.g. 0.1.2.\n\n\n\n\n\nUsing the fledge:: R package for semantic versioning and to document preregistration changes"
  },
  {
    "objectID": "adaptive_preregistration_implementation.html#footnotes",
    "href": "adaptive_preregistration_implementation.html#footnotes",
    "title": "Implementing Adaptive Preregistration with Git and Github",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nhttps://docs.github.com/en/free-pro-team@latest/github/getting-started-with-github/github-glossary#commit-id↩︎\nThe semantic version is a 3-part numeric identifier separated by points, e.g.: 0.0.1, wherein the first number sequences a major version of the preregistration, the second number sequences a minor change, and the third number sequences small changes or patches, yielding: MAJOR.MINOR.PATCH. See https://semver.org/ for further information. Further detail on how to use semantic versioning with adaptive preregistration is summarised in Table 1.↩︎\nhttps://guides.github.com/features/issues/↩︎\nInstructions for creating releases are here: https://help.github.com/en/github/administering-a-repository/managing-releases-in-a-repository↩︎"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This site is a practical guide to accompany the paper:\n\nGould, E., Jones, C.J., Fraser, H.F., Rumpff, L., Fidler, F.M. (in prep.) “But I can’t preregister my research: Improving the reproducibility and transparency of ecology modelling with adaptive preregistration.” TBD.\n\n\n\n\n Back to top"
  }
]